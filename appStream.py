import streamlit as st
import asyncio
from openai import AsyncOpenAI
from datetime import timedelta
from openai.types.responses import ResponseTextDeltaEvent # Added
from agents import Agent, OpenAIChatCompletionsModel, Runner, set_tracing_disabled, ModelSettings # Added Agent, Runner, ModelSettings
from agents.mcp import MCPServerStreamableHttp
# nest_asyncio.apply()
from functools import partial
import backoff

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="MCP æ™ºèƒ½åŠ©æ‰‹", # Changed title
    page_icon="ğŸ¤–",       # Changed icon
    layout="wide"
)

# æ·»åŠ æ ‡é¢˜å’Œè¯´æ˜
st.title("ğŸ¤– MCP æ™ºèƒ½åŠ©æ‰‹") # Changed title
st.markdown("""
æœ¬åŠ©æ‰‹å¯ä»¥è¿æ¥åˆ°æŒ‡å®šçš„ MCP æœåŠ¡å™¨ï¼ŒæŸ¥è¯¢å…¶å¯ç”¨å·¥å…·ï¼Œå¹¶å…è®¸æ‚¨é€šè¿‡AIæ™ºèƒ½ä½“ä¸ä¹‹äº¤äº’æ¥è§£å†³é—®é¢˜ã€‚
""")

# é…ç½®APIå®¢æˆ·ç«¯ (ä¿ç•™)
@st.cache_resource
def get_openai_client():
    return AsyncOpenAI(
        base_url="https://api.deepseek.com/v1",
        api_key="sk-43c7996606b341c790eb888fedaad049", # è¯·æ›¿æ¢ä¸ºæ‚¨çš„æœ‰æ•ˆAPIå¯†é’¥
        timeout=60.0
    )

@st.cache_resource
def get_qwen_client():
    return AsyncOpenAI(
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        api_key="sk-5a75c9c5920146488f518469d6dee532", # è¯·æ›¿æ¢ä¸ºæ‚¨çš„æœ‰æ•ˆAPIå¯†é’¥
        timeout=60.0
    )

@st.cache_resource
def get_siliconflow_client():
    return AsyncOpenAI(
        base_url="https://api.siliconflow.cn/v1",
        api_key="sk-owuhchfalamktrcqsbhhuuxcgfdwcwekrrggmpoomggifotn", # è¯·æ›¿æ¢ä¸ºæ‚¨çš„æœ‰æ•ˆAPIå¯†é’¥
        timeout=60.0
    )

# MCP å·¥å…·æŸ¥è¯¢å‡½æ•° (ä¿ç•™)
async def fetch_mcp_tools(mcp_url: str):
    """
    Connects to the MCP server and lists available tools.
    """
    async with MCPServerStreamableHttp(
        params={
            "url": mcp_url,
            "timeout": timedelta(seconds=60),
            "sse_read_timeout": timedelta(seconds=300)
        }
    ) as mcp_server:
        await mcp_server.connect()
        tools = await mcp_server.list_tools()
        return tools

# æ™ºèƒ½ä½“å“åº”ç”Ÿæˆå™¨
async def generate_agent_response_stream(
    user_query_param: str,
    selected_api_provider_param: str,
    mcp_url_param: str
):
    """
    Sets up the agent, connects to MCP, runs the query, and yields response chunks.
    """
    # 1. Select LLM client
    if selected_api_provider_param == "Qwen":
        selected_model_name = "qwen-turbo"
        selected_client = get_qwen_client()
    elif selected_api_provider_param == "DeepSeek":
        selected_model_name = "deepseek-chat"
        selected_client = get_openai_client()
    elif selected_api_provider_param == "SiliconFlow":
        selected_model_name = "Qwen/Qwen3-235B-A22B"
        selected_client = get_siliconflow_client()
    else:
        yield f"é”™è¯¯ï¼šæœªçŸ¥çš„APIæœåŠ¡å•† '{selected_api_provider_param}'. å°†é»˜è®¤ä½¿ç”¨DeepSeekã€‚\n"
        selected_model_name = "deepseek-chat"
        selected_client = get_openai_client()

    # 2. Set up MCP Server
    try:
        async with MCPServerStreamableHttp(
            params={
                "url": mcp_url_param,
                "timeout": timedelta(seconds=300),
                "sse_read_timeout": timedelta(seconds=300)
            }
        ) as mcp_server_instance:
            connect_attempts = 3
            for attempt in range(connect_attempts):
                try:
                    yield f"æ­£åœ¨è¿æ¥ MCP æœåŠ¡å™¨ (å°è¯• {attempt + 1}/{connect_attempts})...\n"
                    await mcp_server_instance.connect()
                    yield "MCP æœåŠ¡å™¨è¿æ¥æˆåŠŸ!\n"
                    break
                except Exception as connect_err:
                    yield f"MCP è¿æ¥å°è¯• {attempt + 1} å¤±è´¥: {connect_err}\n"
                    if attempt < connect_attempts - 1:
                        await asyncio.sleep(min(2 ** attempt, 5))
                    else:
                        yield f"è¿æ¥ MCP æœåŠ¡å™¨ {mcp_url_param} å¤±è´¥ {connect_attempts} æ¬¡ã€‚è¯·æ£€æŸ¥æœåŠ¡çŠ¶æ€å’Œåœ°å€ã€‚\n"
                        return

            agent = Agent(
                name="MCPToolAgent",
                instructions="ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚è¯·ä½¿ç”¨ä½ å¯ç”¨çš„å·¥å…·æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚",
                model=OpenAIChatCompletionsModel(
                    model=selected_model_name,
                    openai_client=selected_client
                ),
                mcp_servers=[mcp_server_instance],
                model_settings=ModelSettings(tool_choice="auto")
            )

            # 4. Run agent and stream results
            yield "æ™ºèƒ½ä½“æ­£åœ¨å¤„ç†è¯·æ±‚...\n"
            result_stream_handler = Runner.run_streamed(agent, user_query_param)
            
            async for event in result_stream_handler.stream_events():
                if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
                    if hasattr(event.data, 'delta') and event.data.delta is not None:
                        yield event.data.delta
                elif event.type == "tool_calls":
                    for tool_call in event.data.tool_calls:
                         yield f"\næ­£åœ¨è°ƒç”¨å·¥å…·: `{tool_call.name}` (å‚æ•°: `{tool_call.arguments}`)\n"
                elif event.type == "tool_outputs":
                     for tool_output in event.data.outputs:
                         yield f"\nå·¥å…· `{tool_output.name}` è¿”å›: `{tool_output.output}`\n"
                elif event.type == "final_output_event":
                    if event.data.final_output:
                         yield f"\næœ€ç»ˆè¾“å‡º: {event.data.final_output}\n"
    except Exception as e:
        yield f"\nå¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}\n"
        import traceback
        yield traceback.format_exc()

# ç¦ç”¨è·Ÿè¸ª
set_tracing_disabled(disabled=True)

# --- UI Layout ---
with st.sidebar:
    st.header("âš™ï¸ é…ç½®")
    mcp_server_url = st.text_input("MCP æœåŠ¡å™¨åœ°å€", value="http://0.0.0.0:3000/mcp")
    model_options = ["Qwen", "DeepSeek", "SiliconFlow"]
    selected_model_provider = st.selectbox("é€‰æ‹©å¤§æ¨¡å‹æœåŠ¡å•†:", model_options, index=0)

    if st.button("æŸ¥è¯¢ MCP å·¥å…·åˆ—è¡¨"):
        if mcp_server_url:
            with st.spinner(f"æ­£åœ¨æŸ¥è¯¢ MCP æœåŠ¡å™¨ {mcp_server_url} ä¸Šçš„å·¥å…·..."):
                try:
                    tools = asyncio.run(fetch_mcp_tools(mcp_server_url))
                    if tools:
                        st.success("MCP å¯ç”¨å·¥å…·:")
                        for tool in tools:
                            tool_info = f"- **{tool.name}**"
                            if hasattr(tool, 'description') and tool.description:
                                tool_info += f": {tool.description}"
                            st.markdown(tool_info)
                            if hasattr(tool, 'parameters') and tool.parameters:
                                with st.expander("å‚æ•°è¯¦æƒ…"):
                                    st.json(tool.parameters)
                    else:
                        st.warning("æœªæŸ¥è¯¢åˆ°ä»»ä½•å·¥å…·ã€‚è¯·æ£€æŸ¥æœåŠ¡å™¨åœ°å€æˆ–æ—¥å¿—ã€‚")
                except Exception as e:
                    st.error(f"æŸ¥è¯¢ MCP å·¥å…·å¤±è´¥: {e}")
                    st.info("è¯·ç¡®ä¿ MCP æœåŠ¡å™¨å·²å¯åŠ¨ï¼Œåœ°å€æ­£ç¡®ï¼Œä¸”å®ç°äº† Streamable HTTP åè®®ã€‚")
        else:
            st.warning("è¯·è¾“å…¥ MCP æœåŠ¡å™¨åœ°å€ã€‚")
    
    with st.expander("ä½¿ç”¨è¯´æ˜"):
        st.markdown("""
        ### å¦‚ä½•ä½¿ç”¨
        1.  åœ¨ä¾§è¾¹æ é…ç½® **MCP æœåŠ¡å™¨åœ°å€**ã€‚
        2.  (å¯é€‰) ç‚¹å‡» **æŸ¥è¯¢ MCP å·¥å…·åˆ—è¡¨** æ¥æŸ¥çœ‹æœåŠ¡å™¨æä¾›çš„å·¥å…·ã€‚
        3.  åœ¨ä¾§è¾¹æ é€‰æ‹© **å¤§æ¨¡å‹æœåŠ¡å•†**ã€‚
        4.  åœ¨ä¸»èŠå¤©ç•Œé¢è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œæ™ºèƒ½ä½“å°†å°è¯•ä½¿ç”¨ MCP å·¥å…·æ¥å›ç­”ã€‚
        
        ### æ³¨æ„äº‹é¡¹
        -   ç¡®ä¿æ‚¨çš„ API å¯†é’¥å·²åœ¨ä»£ç ä¸­æ­£ç¡®é…ç½® (get_openai_client, get_qwen_client, get_siliconflow_client)ã€‚
        -   MCP æœåŠ¡å™¨å¿…é¡»æ­£åœ¨è¿è¡Œï¼Œå¹¶ä¸”å¯ä»¥ä»è¿è¡Œæ­¤åŠ©æ‰‹çš„åœ°æ–¹è®¿é—®ã€‚
        -   MCP æœåŠ¡å™¨å¿…é¡»å®ç° Streamable HTTP transport åè®®ã€‚
        """)

st.header("ğŸ’¬ ä¸æ™ºèƒ½ä½“å¯¹è¯")

if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if user_query := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œæ™ºèƒ½ä½“å°†å°è¯•ä½¿ç”¨ MCP å·¥å…·..."):
    st.session_state.messages.append({"role": "user", "content": user_query})
    with st.chat_message("user"):
        st.markdown(user_query)

    with st.chat_message("assistant"):
        try:
            stream_generator = generate_agent_response_stream(
                user_query,
                selected_model_provider,
                mcp_server_url
            )
            full_response = st.write_stream(stream_generator)
            if full_response:
                 st.session_state.messages.append({"role": "assistant", "content": full_response})
            # If full_response is None (e.g. generator yielded nothing), we don't add empty message.

        except Exception as e:
            error_message = f"å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºé”™: {str(e)}"
            st.error(error_message)
            st.session_state.messages.append({"role": "assistant", "content": error_message})

st.markdown("---")
st.markdown("Made with â¤ï¸")