import streamlit as st
import asyncio
from openai import AsyncOpenAI
from openai.types.responses import ResponseTextDeltaEvent
from agents import Agent, OpenAIChatCompletionsModel, Runner, set_tracing_disabled, trace # æ·»åŠ  trace å¯¼å…¥
from agents.mcp import MCPServerSse
from agents.model_settings import ModelSettings
# nest_asyncio.apply() # Temporarily comment out to check context var issues
from functools import partial
import backoff

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="åœ°ç†ä¿¡æ¯åŠ©æ‰‹",
    page_icon="ğŸ—ºï¸",
    layout="wide"
)

# æ·»åŠ æ ‡é¢˜å’Œè¯´æ˜
st.title("ğŸ—ºï¸ æ™ºèƒ½åœ°ç†ä¿¡æ¯åŠ©æ‰‹")
st.markdown("""
è¿™æ˜¯ä¸€ä¸ªæ™ºèƒ½åœ°ç†ä¿¡æ¯åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©æ‚¨ï¼š
- æŸ¥è¯¢å¤©æ°”ä¿¡æ¯
- è·å–åœ°å€è¯¦æƒ…
- è§„åˆ’æ—…è¡Œè·¯çº¿
- æ›´å¤šåœ°ç†ç›¸å…³æŸ¥è¯¢...
""")

# é…ç½®DeepSeek API
@st.cache_resource
def get_openai_client():
    return AsyncOpenAI(
        base_url="https://api.deepseek.com/v1",
        api_key="sk-43c7996606b341c790eb888fedaad049",
        timeout=60.0
    )

@st.cache_resource
def get_qwen_client():
    return AsyncOpenAI(
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
        api_key="sk-5a75c9c5920146488f518469d6dee532",
        timeout=60.0
    )

@backoff.on_exception(backoff.expo, Exception, max_tries=3)
async def run_streamed_with_retry(agent, query):
    return Runner.run_streamed(agent, query)

async def generate_agent_response_stream(user_query_param: str):
    """
    Sets up the agent, connects to MCP, runs the query, and yields response chunks.
    This function is designed to be used with st.write_stream.
    """
    # è®¾ç½®ä¸º True ä½¿ç”¨ Qwen API, è®¾ç½®ä¸º False ä½¿ç”¨ DeepSeek API
    USE_QWEN_API = True 

    if USE_QWEN_API:
        selected_model_name = "qwen-turbo"
        selected_client = get_qwen_client()
        # UI updates like st.info should ideally be outside the generator if they need to appear before streaming.
        # However, for simplicity in this refactor, keeping some logic here.
        # Consider moving pre-stream UI updates to the button click handler.
    else:
        selected_model_name = "deepseek-chat"
        selected_client = get_openai_client()

    # Note: st.info calls within an async generator passed to st.write_stream might not behave as expected
    # regarding timing with the spinner. It's generally better to do initial UI updates before st.write_stream.
    # For this example, we'll proceed, but be mindful of this in complex apps.

    # Temporarily remove the trace context manager to isolate the ContextVar issue
    # with trace("Geographic Info Query Workflow", metadata={"user_query": user_query_param}):
    async with MCPServerSse(
        name="Amap Maps Server",
        params={
            "url": "https://mcp-e7501f2d-826a-4be5.api-inference.modelscope.cn/sse",
            "timeout": 60  # Increased from 30 to 60 seconds
        }
    ) as mcp_server:
        try:
            # Consider moving st.info/success messages for connection to before st.write_stream
            # await mcp_server.connect()

            connect_attempts = 3
            connect_success = False
            last_connect_error = None
            for attempt in range(connect_attempts):
                try:
                    # st.info(f"Attempting to connect to MCP server (attempt {attempt + 1}/{connect_attempts})...")
                    await mcp_server.connect()
                    # st.success("MCP Server connected successfully!")
                    connect_success = True
                    break # Exit loop on success
                except Exception as connect_err:
                    # st.warning(f"MCP connection attempt {attempt + 1} failed: {str(connect_err)}")
                    last_connect_error = connect_err
                    if attempt < connect_attempts - 1:
                        await asyncio.sleep(min(2 ** attempt, 10)) # Exponential backoff: 1, 2, 4, capped at 10s
                    else:
                        # st.error("Failed to connect to MCP server after multiple attempts.")
                        print(f"Failed to connect to MCP server after {connect_attempts} attempts. Last error: {last_connect_error}")
                        raise # Re-raise the last exception to be caught by the outer handler
            
            if not connect_success:
                # This path should ideally not be reached if the raise above works, but as a failsafe.
                # st.error("MCP Server connection failed definitively after retries.")
                # We re-raised in the loop, so this part might not be strictly necessary
                # depending on how st.write_stream handles exceptions from the generator.
                # For clarity, ensure an exception is raised if connection fails.
                if last_connect_error:
                    raise Exception(f"Failed to connect to MCP server after {connect_attempts} attempts.") from last_connect_error
                else:
                    raise Exception(f"Failed to connect to MCP server after {connect_attempts} attempts for an unknown reason.")

            agent = Agent(
                name="Assistant",
                instructions="æˆ‘æ˜¯ä¸€ä¸ªå¯ä»¥å¸®åŠ©æŸ¥è¯¢åœ°ç†ä¿¡æ¯çš„åŠ©æ‰‹,æˆ‘å¯ä»¥å¸®ä½ æŸ¥è¯¢å¤©æ°”ã€åœ°å€ã€è·¯çº¿ç­‰ä¿¡æ¯ã€‚",
                model=OpenAIChatCompletionsModel(
                    model=selected_model_name, 
                    openai_client=selected_client
                ),
                mcp_servers=[mcp_server],
                model_settings=ModelSettings(tool_choice="required") 
                # Note: tool_choice="required" means the agent MUST use a tool.
                # Streaming text deltas (ResponseTextDeltaEvent) typically happens
                # when the LLM generates a direct text response or the text part of a response after tool use.
                # Ensure the agent's workflow and the events from Runner.run_streamed align with this expectation.
            )
            
            result_stream_handler = await run_streamed_with_retry(agent, user_query_param)
            
            async for event in result_stream_handler.stream_events():
                if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
                    # Based on user's example: event.data is ResponseTextDeltaEvent and has a 'delta' attribute
                    if hasattr(event.data, 'delta') and event.data.delta is not None:
                        yield event.data.delta
                # Other event types (tool_calls, tool_outputs, final_output_event) could be handled here
                # if more detailed streaming (e.g., "using tool X...") is needed.
                # For now, focusing on streaming the text delta as per the example.

        except Exception as e:
            # Handle exceptions that occur within the generator
            # This error might not be directly visible in st.error in the main UI easily
            # st.write_stream will raise the exception on the main thread.
            print(f"Error in stream generator: {str(e)}") # Log error
            # Re-raise to be caught by st.write_stream's error handling
            # or the try-except block in the button click handler
            raise 

# Removed run_async function as st.write_stream handles async generator execution.

# ç¦ç”¨è·Ÿè¸ª
set_tracing_disabled(disabled=True) # Disable tracing

# åˆ›å»ºè¾“å…¥æ¡†
user_query = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜", placeholder="ä¾‹å¦‚ï¼šåŒ—äº¬çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿæˆ–è€… åŒ—äº¬åˆ°ä¸Šæµ·æ€ä¹ˆèµ°ï¼Ÿ")

# åˆ›å»ºå‘é€æŒ‰é’®
if st.button("å‘é€", type="primary"):
    if user_query:
        # Display initial status messages before starting the stream
        st.info(f"æ”¶åˆ°é—®é¢˜ï¼š'{user_query}'. å¼€å§‹å¤„ç†...")
        if True: # Simplified from USE_QWEN_API for this example message
            st.info("å½“å‰ä½¿ç”¨ Qwen API")
        else:
            st.info("å½“å‰ä½¿ç”¨ DeepSeek API")
        st.info("å‡†å¤‡è¿æ¥åˆ° MCP æœåŠ¡å™¨...")

        with st.spinner("æ­£åœ¨æ€è€ƒä¸­..."):
            try:
                # Directly use st.write_stream with the async generator function
                st.write("å›ç­”ï¼š") # Label for the answer area
                # Pass the user_query to the generator function
                st.write_stream(generate_agent_response_stream(user_query))
                st.success("æŸ¥è¯¢å®Œæˆï¼") # Appears after the stream finishes
            except Exception as e:
                st.error(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
    else:
        st.warning("è¯·è¾“å…¥é—®é¢˜åå†å‘é€")

# æ·»åŠ ä½¿ç”¨è¯´æ˜
with st.expander("ä½¿ç”¨è¯´æ˜"):
    st.markdown("""
    ### å¦‚ä½•ä½¿ç”¨
    1. åœ¨è¾“å…¥æ¡†ä¸­è¾“å…¥æ‚¨çš„é—®é¢˜
    2. ç‚¹å‡»"å‘é€"æŒ‰é’®
    3. ç­‰å¾…ç³»ç»Ÿå“åº”
    
    ### ç¤ºä¾‹é—®é¢˜
    - åŒ—äº¬ä»Šå¤©çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ
    - åŒ—äº¬åˆ°ä¸Šæµ·çš„é«˜é“è·¯çº¿
    - æ­å·è¥¿æ¹–é™„è¿‘çš„æ™¯ç‚¹
    - ä¸Šæµ·ä¸œæ–¹æ˜ç çš„å…·ä½“åœ°å€
    """)

# æ·»åŠ é¡µè„š
st.markdown("---")
st.markdown("Made with â¤ï¸ by Your Team")